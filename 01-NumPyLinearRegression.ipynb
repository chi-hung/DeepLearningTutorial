{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本筆記內容：利用批次梯度下降法(Mini-batch Gradient Descent)實作線性迴歸。\n",
    "\n",
    "我們希望讀者能透過此範例，來理解監督式機器學習的基本概念，包含：\n",
    "   * 監督式機器學習即是最小化Loss函數。此範例中，Loss函數為Least Square Loss。\n",
    "   * 如何利用Mini-batch Gradient Descent更新權重。\n",
    "   * L1(Lasso)的功用。\n",
    "---\n",
    "   \n",
    "# 索引\n",
    "[1.定義資料載入以及權重更新函數](#1.-定義資料載入以及權重更新函數)\n",
    "\n",
    "[2. 定義專門用來處理線性迴歸的類別](#2.-定義專門用來處理線性迴歸的類別)\n",
    "\n",
    "[3. 準備測試資料](#3.-準備測試資料)\n",
    "\n",
    "[4. 開始對資料做線性回歸](#4.-開始對資料做線性回歸)\n",
    "\n",
    "[5. 改變C, 由10000至10, 看回歸結果如何變化](#5.-改變C,-由10000至10,-看回歸結果如何變化)\n",
    "  * [5.a. 將回歸結果繪圖](#5.a.-將回歸結果繪圖)\n",
    "  * [5.b. 將權重繪圖](#5.b.-將權重繪圖)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad   # 此套件幫我們將Loss取gradients\n",
    "import autograd.numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 定義資料載入以及權重更新函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataLoader(x,y,batch_size,shuffle=True):\n",
    "    '''此函數負責批次性的拋出資料(資料的量為batch_size)給機器學習的演算法做訓練。'''\n",
    "    \n",
    "    num_batches=y.shape[0]//batch_size\n",
    "    data=np.hstack((x,y.reshape(-1,1)))\n",
    "    \n",
    "    if shuffle==True:\n",
    "        np.random.shuffle(data)\n",
    "    for j in range(num_batches):\n",
    "        x_batch= data[batch_size*j:batch_size*(j+1),0:x.shape[1]]\n",
    "        y_batch= data[batch_size*j:batch_size*(j+1),x.shape[1]]\n",
    "        yield x_batch,y_batch\n",
    "\n",
    "def SGD(weights,weights_grads,lr=0.01):\n",
    "    '''此函數用來做權重更新(stochastic gradient descent)。'''\n",
    "#     a-=b\n",
    "#     a=a-b\n",
    "    weights-=lr*weights_grads\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[回索引](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 定義專門用來處理線性迴歸的類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "    '''此類別處理線性回歸(+L1正規項)。'''\n",
    "    \n",
    "    def predict(self,weights,x):\n",
    "        '''輸入x, 利用現有的回歸參數w[0],w[1],w[2]來預測y。'''\n",
    "        \n",
    "        y_pred=np.dot(x,weights[1:])+weights[0]\n",
    "        return y_pred\n",
    "    \n",
    "    def leastSquareLoss(self,weights):\n",
    "        '''此為最小平方法的Loss函數(添加L1規範項)。註：參數C越大，則規範項強度越小。'''\n",
    "        \n",
    "        y_pred=self.predict(weights,self.x_batch)\n",
    "        loss=(1./2.)*np.mean( (self.y_batch-y_pred)**2 )\n",
    "        loss+=(1/self.C)*np.sum(np.abs(weights[1:])) # the L1 term\n",
    "        return loss\n",
    "\n",
    "    def fit(self,x,y,weights,batch_size,C,num_epochs,lr=0.001,infoStep=1,verbose=True):\n",
    "        '''以批次更新方式，尋找最適回歸參數。'''\n",
    "        \n",
    "        # 初始化參數和用來儲存batch的資料矩陣\n",
    "        self.C=C\n",
    "        self.x_batch=np.zeros((batch_size,x.shape[1]))\n",
    "        self.y_batch=np.zeros(batch_size)\n",
    "        \n",
    "        # 得到Loss函數對weights偏微分的函數表示式 (L')\n",
    "        gradLoss=grad(self.leastSquareLoss)\n",
    "\n",
    "        # 開始更新weights, 以降低當前Loss\n",
    "        for epoch in range(1,num_epochs+1):\n",
    "            generator=dataLoader(x,y,batch_size,shuffle=False)\n",
    "            for idx,(x_batch,y_batch) in enumerate(generator):\n",
    "                self.x_batch=x_batch\n",
    "                self.y_batch=y_batch\n",
    "                \n",
    "                if(epoch==1 and idx==0):\n",
    "                    # 根據初始weights來得到當前Loss的值\n",
    "                    print('initial loss=',self.leastSquareLoss(weights))\n",
    "                \n",
    "                # 得到 L'(weights)\n",
    "                weights_grads=gradLoss(weights)\n",
    "                # 以梯度下降法更新weights\n",
    "                weights=SGD(weights,weights_grads,lr=lr)\n",
    "            if(verbose==True and epoch%infoStep==0):\n",
    "                loss=self.leastSquareLoss(weights)\n",
    "                print('epoch %3i,'%epoch,'loss=',loss)\n",
    "            elif (verbose==False and epoch==num_epochs):\n",
    "                loss=self.leastSquareLoss(weights)\n",
    "                print('epoch %3i,'%epoch,'loss=',loss)\n",
    "        return weights,loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[回索引](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 準備測試資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 準備x,y\n",
    "x = np.array([i*np.pi/180 for i in range(1,100,2)])\n",
    "np.random.seed(100)  #Setting seed for reproducability\n",
    "y = np.sin(x) + np.random.normal(0,0.05,len(x))\n",
    "# 將x,y存成Pandas資料表型態\n",
    "data = pd.DataFrame(np.column_stack([y,x]),columns=['y','x'])\n",
    "plt.plot(data['x'],data['y'],'.')\n",
    "plt.show()\n",
    "# 準備x^2,x^3,x^4次方項\n",
    "for i in range(2,5):\n",
    "    colname = 'x_%d'%i\n",
    "    data[colname] = data['x']**i\n",
    "# 整理好資料後，將其存回Numpy矩陣\n",
    "y=data.iloc[:,0].values\n",
    "x=data.iloc[:,1:].values\n",
    "data.head(3)\n",
    "# 檢查x,y矩陣的形狀為何\n",
    "print('shape of x=',x.shape)\n",
    "print('shape of y=',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[回索引](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 開始對資料做線性回歸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a. L1規範項可忽略 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "num_epochs=1000\n",
    "infoStep=100\n",
    "lr=0.001\n",
    "C=10000  # C非常大，L1規範項可忽略\n",
    "verbose=True\n",
    "\n",
    "regressor= LinearRegression()\n",
    "weights= np.random.normal(0,0.01,x.shape[1]+1) # 初始化權重向量\n",
    "weights,loss= regressor.fit(x,y,weights,batch_size,C,num_epochs,\n",
    "                            lr=lr,infoStep=infoStep,verbose=verbose) # 回歸開始\n",
    "print('\\n final weights=',weights)\n",
    "y_pred=regressor.predict(weights,x) # 以所學之權重向量，來預測所輸入的x其y值為何\n",
    "\n",
    "plt.figure(dpi=80)\n",
    "plt.scatter(x[:,0],y,label='data') # 繪製原始資料\n",
    "plt.plot(x[:,0],y_pred,label='fitting curve',ls='-',color='purple') # 繪製回歸結果\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[回索引](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.b. 將C的數值減低，以增加L1規範項的影響"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "num_epochs=1000\n",
    "infoStep=100\n",
    "lr=0.01\n",
    "C=50  # C不是非常大，L1規範項不可忽略\n",
    "verbose=True\n",
    "\n",
    "regressor= LinearRegression()\n",
    "weights= np.random.normal(0,0.01,x.shape[1]+1)\n",
    "weights,loss= regressor.fit(x,y,weights,batch_size,C,num_epochs,\n",
    "                            lr=lr,infoStep=infoStep,verbose=verbose)\n",
    "print('\\n final weights=',weights)\n",
    "y_pred=regressor.predict(weights,x)\n",
    "\n",
    "plt.figure(dpi=80,figsize=(6,4))\n",
    "plt.scatter(x[:,0],y,label='data')\n",
    "plt.plot(x[:,0],y_pred,label='fitting curve',ls='-',color='purple')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[回索引](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 改變C, 由10000至10, 看回歸結果如何變化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info=[]\n",
    "for C in np.logspace(4,1,6,dtype=np.int32):\n",
    "    print('\\n C=',C)\n",
    "    batch_size=10\n",
    "    num_epochs=3000\n",
    "    infoStep=100\n",
    "    lr=0.01\n",
    "    verbose=False\n",
    "\n",
    "    regressor= LinearRegression()\n",
    "    weights= np.random.normal(0,0.01,x.shape[1]+1)\n",
    "    weights,loss= regressor.fit(x,y,weights,batch_size,C,num_epochs,\n",
    "                                lr=lr,infoStep=infoStep,verbose=verbose)\n",
    "    y_pred=regressor.predict(weights,x)\n",
    "    \n",
    "    info.append([weights,loss,y_pred,C])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[回索引](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a. 將回歸結果繪圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(2,3,sharey=True,sharex=True)\n",
    "fig.set_size_inches(10,5)\n",
    "fig.dpi=120\n",
    "axes=axes.reshape(-1)\n",
    "for idx,result in enumerate(info):\n",
    "    \n",
    "    weights=result[0]\n",
    "    loss=result[1]\n",
    "    y_pred=result[2]\n",
    "    C=result[3]\n",
    "\n",
    "    axes[idx].scatter(x[:,0],y,label='data')\n",
    "    axes[idx].plot(x[:,0],y_pred,label='fitting curve',ls='-',color='purple')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].set_title('$c=%i, loss=%.3f$'%(C,loss))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[回索引](#索引)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.b. 將權重繪圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=np.zeros((6,5))\n",
    "c=np.zeros(6,dtype=np.int32)\n",
    "for idx,result in enumerate(info):\n",
    "    weights=result[0]\n",
    "    C=result[3]\n",
    "    w[idx,:]=weights\n",
    "    c[idx]=C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "fig.set_dpi(200)\n",
    "\n",
    "s=sns.cubehelix_palette(reverse=True,as_cmap=True)\n",
    "sns.heatmap(w,xticklabels=['$const$','$x$','$x^2$','$x^3$','$x^4$']\n",
    "             ,yticklabels=c,cmap=s,ax=ax,annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[回索引](#索引)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
