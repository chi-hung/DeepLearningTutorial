{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='00'> 使用DenseNet做模型訓練 </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本練習使用CIFAR10資料集：https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import cv2\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.optimizers import SGD,Adadelta\n",
    "from keras.callbacks import LearningRateScheduler,EarlyStopping,ModelCheckpoint,LambdaCallback,CSVLogger\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from densenet import MY_DenseNet\n",
    "\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =====================================================================\n",
    "# # 由於課堂上可能有多人共用同一顆GPU，以下限定使用者只能用計算卡上面一半的記憶體。\n",
    "# import tensorflow as tf\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5 # 使用一半記憶體\n",
    "# set_session(tf.Session(config=config))\n",
    "# # ====================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='01'>載入圖片至電腦記憶體 </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先看一下包含資料集的資料夾有什麼內容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -hl ../datasets/cifar-10-batches-py/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_batch_1, data_batch_2,..data_batch_5以及test_batch是以binary的方式儲存在硬碟裡。以下我們寫幾個函數，用以載入這些binary格式的圖檔至電腦內的記憶體中，並且將圖的以矩陣的方式儲存。這些圖矩陣的shape為(Number of figures,Width,Height,Channel)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(fpath):\n",
    "    \"\"\"This function extract a batch of CIFAR10 data\n",
    "       from the chosen binary file.\n",
    "       This function is a simplified version of\n",
    "       https://github.com/keras-team/keras/blob/master/keras/datasets/cifar.py\n",
    "    \"\"\"\n",
    "    with open(fpath, 'rb') as f:\n",
    "        d = pickle.load(f, encoding='bytes')\n",
    "        # Keys are in the \"byte\" format. Let's decode them into utf8 strings.\n",
    "        d_decoded = {}\n",
    "        for k, v in d.items():\n",
    "            d_decoded[k.decode('utf8')] = v\n",
    "        d = d_decoded\n",
    "    data = d['data']\n",
    "    labels = d['labels']\n",
    "    data = data.reshape(data.shape[0], 3, 32, 32)\n",
    "    data = data.transpose(0,2,3,1)\n",
    "    return data,labels\n",
    "\n",
    "def load_data(path):\n",
    "    '''\n",
    "    載入以binary方式儲存的影像至電腦內記憶體。\n",
    "    '''\n",
    "    num_train_samples = 50000\n",
    "\n",
    "    x_train = np.zeros((num_train_samples, 32,32,3), dtype='uint8')\n",
    "    y_train = np.zeros((num_train_samples,), dtype='uint8')\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        fpath = os.path.join(path, 'data_batch_') + str(i)\n",
    "        data, labels = load_batch(fpath)\n",
    "        x_train[(i - 1) * 10000:i * 10000, :, :, :] = data\n",
    "        y_train[(i - 1) * 10000:i * 10000] = labels\n",
    "\n",
    "    fpath = os.path.join(path, 'test_batch')\n",
    "    x_test, y_test = load_batch(fpath)\n",
    "\n",
    "    return (x_train, y_train), (np.array(x_test), np.array(y_test,dtype=\"uint8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test)=load_data('../datasets/cifar-10-batches-py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape,y_train.shape)\n",
    "print(x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is a cell for testing. It reduces the data samples to 10% of the original dataset.\n",
    "# # Comment out this cell if you'd like to use the whole dataset for training.\n",
    "# x_train=x_train[:5000]\n",
    "# y_train=y_train[:5000]\n",
    "# x_test=x_test[:1000]\n",
    "# y_test=y_test[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上，我們得到了x_train, x_test, y_train,y_test四個放置圖片的矩陣，其shape均為(Number of figures,Width,Height,Channel)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著，我們抽出幾張圖來看，稍微了解一下這些資料大概的樣貌："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datasets/cifar-10-batches-py/labels.txt\") as reader:\n",
    "    fig_labels=reader.read()\n",
    "fig_labels=fig_labels.split(\"\\n\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_label={}\n",
    "for idx,fig_labels in enumerate(fig_labels):\n",
    "        idx_to_label[idx]=fig_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#隨機抽取12張圖來看一下\n",
    "num_figures_display=12\n",
    "fig_indexes=np.random.choice(x_train.shape[0],num_figures_display)\n",
    "\n",
    "fig,axes=plt.subplots(2,6)\n",
    "for fig_idx,axis in zip(fig_indexes,axes.reshape(-1) ):\n",
    "    axis.axis('off')\n",
    "    axis.imshow(x_train[fig_idx])\n",
    "    axis.set_title(idx_to_label[ y_train[fig_idx] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[回索引](#%E6%9C%AC%E7%AD%86%E8%A8%98%E7%9A%84%E5%85%A7%E5%AE%B9%E5%A6%82%E4%B8%8B%EF%BC%9A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='02'> 將圖片做一些預處理 </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 做normalization\n",
    "data_type=np.float32\n",
    "n_channels=3\n",
    "\n",
    "x_train=x_train.astype(data_type)\n",
    "x_test=x_test.astype(data_type)\n",
    "\n",
    "for i in range(n_channels):\n",
    "    mean = np.mean(x_train[:, :, :, i])\n",
    "    std = np.std(x_train[:, :, :, i])\n",
    "    x_train[:, :, :, i] = ( (x_train[:, :, :, i] - mean) / std )\n",
    "    x_test[:, :, :, i] =  ( (x_test[:, :, :, i] - mean) / std )\n",
    "# 模型只看過train data, 故只能用train data來計算mean, std。 做資料驗證時，test data也需要用相同的mean, std來將其轉換。\n",
    "\n",
    "# # 另一個簡單的normalization方式，是將x直接除以255，使得x內的所有值均分佈於[0,1]之間。\n",
    "# x_train=(x_train/255. ).astype(data_type)\n",
    "# x_test=(x_test/255. ).astype(data_type)\n",
    "\n",
    "# 將y轉換成為one hot的形式\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=10).astype(np.uint8)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=10).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell checks if the data is normalized properly.\n",
    "for i in range(n_channels):\n",
    "    print( \"mean of the normalized x_train (channel %i): %s\"%(i,np.mean(x_train[:, :, :, i] ) ) )\n",
    "    print( \"mean of the normalized x_test (channel %i): %s\"%(i,np.mean(x_test[:, :, :, i] ) ) )\n",
    "    print()\n",
    "    print( \"std of the normalized x_test (channel %i): %s\"%(i,np.std(x_test[:, :, :, i] ) ) )\n",
    "    print( \"std of the normalized x_test (channel %i): %s\"%(i,np.std(x_test[:, :, :, i] ) ) )\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[回索引](#%E6%9C%AC%E7%AD%86%E8%A8%98%E7%9A%84%E5%85%A7%E5%AE%B9%E5%A6%82%E4%B8%8B%EF%BC%9A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='03'> 建立並訓練模型 </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ======= 模型訓練參數設定 =======\n",
    "num_epochs=300\n",
    "\n",
    "# num_gpus=1\n",
    "batch_size=128\n",
    "learning_rate=0.1\n",
    "\n",
    "# DenseNet-BC-L40-K48\n",
    "block_layers=[6,6,6]\n",
    "growth_rate=48\n",
    "\n",
    "first_conv_params={\"fsize\":(3,3),\"stride\":1,\"maxpool\":False}\n",
    "\n",
    "weight_decay=1.E-4\n",
    "dropout_rate=0.2\n",
    "# ==============================\n",
    "\n",
    "# ==== learning rate排程設定 ====\n",
    "def schedule(epoch):\n",
    "    if 150 <= epoch < 225:\n",
    "        lr=0.01\n",
    "    elif epoch >= 225:\n",
    "        lr=0.001\n",
    "    else:\n",
    "        lr=0.1\n",
    "    return lr\n",
    "# ==============================\n",
    "\n",
    "# ======== 模型儲存設定 ==========\n",
    "prefix='DenseNetBC-CIFAR10-L40-K48-SGD-BS128'\n",
    "model_save_path='../checkpoints/'+ prefix +'-{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "model_save_interval=100\n",
    "csv_log_every=1\n",
    "log_filename= 'log-'+prefix+'.csv'\n",
    "# ==============================\n",
    "\n",
    "# 設定訓練途中存擋，以及更改learning rate\n",
    "lr_decay_scheduler = LearningRateScheduler(schedule)\n",
    "# 設定訓練途中訓練情況變糟即停止\n",
    "earlyStopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "checkPointer = ModelCheckpoint(model_save_path, \n",
    "                               monitor='val_loss', \n",
    "                               verbose=1,\n",
    "                               save_best_only=False,\n",
    "                               save_weights_only=False,\n",
    "                               mode='auto', \n",
    "                               period=model_save_interval)\n",
    "# 設定每隔一陣子就將訓練情形輸出至一個csv檔\n",
    "# csv_logging_callback = CSVLogger(log_filename)\n",
    "pd.DataFrame(OrderedDict([(\"epoch\",[]),(\"loss\",[]),(\"acc\",[]),(\"val_loss\",[]),(\"val_acc\",[])]) ) \\\n",
    "            .to_csv(log_filename,index=None,mode='w')\n",
    "def csv_update(epoch,logs):\n",
    "    if epoch%csv_log_every==0:\n",
    "        paramsDict=OrderedDict()\n",
    "        paramsDict.update({'epoch':epoch})\n",
    "        for col in ['loss','acc','val_loss','val_acc']:\n",
    "            paramsDict.update({col:[logs[col],]})\n",
    "        pd.DataFrame(paramsDict).to_csv(log_filename,index=None,header=None,mode='a')\n",
    "csv_logging_callback = LambdaCallback(\n",
    "    on_epoch_end=csv_update )\n",
    "\n",
    "# 取得模型\n",
    "model=MY_DenseNet(in_shape=(32,32,3),\n",
    "                  out_classes=10,\n",
    "                  weight_decay=weight_decay,\n",
    "                  growth_rate=growth_rate,\n",
    "                  block_layers=block_layers,\n",
    "                  first_conv_params=first_conv_params,\n",
    "                  dropout_rate=dropout_rate\n",
    "                 \n",
    "                 ).build_model()\n",
    "\n",
    "#parallel_model = multi_gpu_model(model, gpus=num_gpus)\n",
    "# 定義模型優化方式\n",
    "opt=SGD(lr=learning_rate,\n",
    "        momentum=0.9,\n",
    "        nesterov=True,\n",
    "        decay=0.)\n",
    "# 編譯模型：給定模型目標和訓練方式\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])\n",
    "# 訓練模型\n",
    "history=model.fit(x_train,y_train_one_hot,\n",
    "                  epochs=num_epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  validation_split=0.1,\n",
    "                  callbacks=[lr_decay_scheduler,checkPointer,csv_logging_callback],\n",
    "                  verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#畫出訓練過程\n",
    "fig,axes=plt.subplots(1,2,figsize=(18,6),dpi=120)\n",
    "axes[0].plot(model.history.history['acc'],ms=5,marker='o',label='train acc',ls='--')\n",
    "axes[0].plot(model.history.history['val_acc'],ms=5,marker='o',label='val acc',ls='--')\n",
    "axes[0].legend()\n",
    "axes[1].plot(model.history.history['loss'],ms=5,marker='o',label='train loss',ls='--')\n",
    "axes[1].plot(model.history.history['val_loss'],ms=5,marker='o',label='val loss',ls='--')\n",
    "axes[1].legend()\n",
    "\n",
    "for idx,ax in enumerate(axes):\n",
    "    ax.set_xlabel('Epoch')\n",
    "    if idx==0:\n",
    "        ax.set_ylabel('Accuracy')\n",
    "    else:\n",
    "        ax.set_ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#畫出訓練過程 (Loss and Acc in the log scale)\n",
    "fig,axes=plt.subplots(1,2,figsize=(18,6),dpi=120)\n",
    "axes[0].semilogy(model.history.history['acc'],ms=5,marker='o',label='train acc',ls='--')\n",
    "axes[0].semilogy(model.history.history['val_acc'],ms=5,marker='o',label='val acc',ls='--')\n",
    "axes[0].legend()\n",
    "axes[1].semilogy(model.history.history['loss'],ms=5,marker='o',label='train loss',ls='--')\n",
    "axes[1].semilogy(model.history.history['val_loss'],ms=5,marker='o',label='val loss',ls='--')\n",
    "axes[1].legend()\n",
    "\n",
    "for idx,ax in enumerate(axes):\n",
    "    ax.set_xlabel('Epoch')\n",
    "    if idx==0:\n",
    "        ax.set_ylabel('Log Accuracy')\n",
    "    else:\n",
    "        ax.set_ylabel('Log Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[回索引](#00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
